# -*- coding: utf-8 -*-
"""Apriori Algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17n23RhrmrHBzF27e-twE9MOFrrTNC-Rt
"""

#reading the data using pandas
import pandas as pd

#importing Data
store_Data = pd.read_csv('/content/store_data.csv')

#showing data
store_Data.head()

# Converting the data to feed the model

final_data = pd.DataFrame(columns=['Transaction', 'Item']) #creating columns
store_Data = store_Data.T #accessing the numPy array

for col in store_Data.columns:
  col_data = list(store_Data[col].dropna()) #removing rows containing null values
  temp_dict = {'Transaction':[int(col)]*len(col_data),
               'Items':col_data}
  temp_df = pd.DataFrame(temp_dict)
  final_data = final_data.append(temp_df,ignore_index=True) # adding the found data the end of the list.

final_data.head()

#Visualing and ploting data
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(15,8)) # for the graph frame
order = final_data['Items'].value_counts()[:10].index #ordering 10 most selling items
sns.countplot(x = 'Items', data=final_data,order = order)
plt.title('Top 10 most selling items')
plt.show()

# Number of unique items and total number of transactions
len(final_data['Items'].unique())
len(final_data['Transaction'].unique())

# Converting the data in the required format

final_data_dummy = pd.get_dummies(final_data['Items']) # data manupulation, converting categorical data into indicator variables.
final_data_dummy['Transaction'] = final_data['Transaction']

# Knowing that product is bought or not using encode_units function

def encode_units(x):
  if x <= 0:
    return 0
  if x >= 1:
    return 1

format_data = final_data_dummy.groupby('Transaction').sum() #spliting the data into groups by transaction
format_data = format_data.applymap(encode_units) #applying the encode_unitd function

format_data.head()

# Applying Apriori algorithm using mlxtend library
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

# Getting support values for each combination

# min_support is the limit of support value
# frequent_itemsets has frequency of each item and its combination

frequent_itemsets  =  apriori(format_data, min_support = 0.01, use_colnames = True)

frequent_itemsets.head()

# Applying association rules to get confidence and lift values

result = association_rules(frequent_itemsets, metric = "confidence", min_threshold=0.01)
result.sort_values('confidence',ascending=False)

result

result.to_csv('result.csv', index=False)

cat result.csv

